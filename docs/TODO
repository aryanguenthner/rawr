ToDo
    (does not include known issues)

        google dorks function not complete (need to build out API function)
		figure correct way to init()
		program control
			possible solution - change queue items to individual tasks rather than one host
			PAUSE functionality (in-program)
			Ctrl+C 'graceful stop' effectiveness
			STOP file (for resuming scans)
                a switch to resume a scan
                <>.q - a dump of the current queue
                this could take the db and pick up where it left off
			need a better timeout mechanism for spider and diagram creation
            dynamic thread count based on RAM availability	
		cleanup
			functions.py
				SiThread
				parsedata - initial cleanup
				crawl
				parse_qualys_scan_report_xml - initial cleanup
				parse_nessus_xml - initial cleanup
				parse_nmap_xml - initial cleanup
		postgres compatibility
		smarter redirects + history
			if res.status_code == 302 and not domainname in res.get_redirect_location:
				target['redir_loc'] = res.get_redirect_location
			else: follow redirect!
			target['history'] to record all redirect urls
		better crawling
			implement spider_url_hit_limit for individual base URLS (without vars)
			need a better timeout mechanism for spider and diagram creation
			reduce hard-limit for # of links in diagram			
		
		serverinfo- XSL or XLSX - center all fields, set widths, freeze top row, top row - white/bold font and dark grey bg	dddddddddddddddddddddddsssss
		burp and faraday plugins
			faraday - serverinfo into a single XML doc
				give them the framework for RAWR's XML format
		stats page in HTML report
        		div after HTML 'data' div
        		stats ideas: server version, SSL, ports, service (PHP, ASP.NET)
		scan comparison functionality - postgres
			compare two scans
			compare a db scan with new scan
			'diff' HTML report
